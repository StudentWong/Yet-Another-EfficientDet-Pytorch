{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EfficientDet Training On A Custom Dataset\n",
    "\n",
    "\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/blob/master/tutorial/train_shape.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on github\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/zylo117/Yet-Another-EfficientDet-Pytorch/blob/master/tutorial/train_shape.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This tutorial will show you how to train a custom dataset.\n",
    "\n",
    "## For the sake of simplicity, we generate a dataset of different shapes, like rectangles, triangles, circles.\n",
    "\n",
    "## Please enable GPU support to accelerate on notebook setting if you are using colab.\n",
    "\n",
    "### 0. Install Requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "!pip install pycocotools numpy==1.16.0 opencv-python tqdm tensorboard tensorboardX pyyaml matplotlib\n",
    "!pip install torch==1.4.0\n",
    "!pip install torchvision==0.5.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Prepare Custom Dataset/Pretrained Weights (Skip this part if you already have datasets and weights of your own)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "if \"projects\" not in os.getcwd():\n",
    "  !git clone --depth 1 https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch\n",
    "  os.chdir('Yet-Another-EfficientDet-Pytorch')\n",
    "  sys.path.append('.')\n",
    "else:\n",
    "  !git pull\n",
    "\n",
    "# download and unzip dataset\n",
    "! mkdir datasets\n",
    "! wget https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.1/dataset_shape.tar.gz\n",
    "! tar xzf dataset_shape.tar.gz\n",
    "\n",
    "# download pretrained weights\n",
    "! mkdir weights\n",
    "! wget https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d0.pth -O weights/efficientdet-d0.pth\n",
    "\n",
    "# prepare project file projects/shape.yml\n",
    "# showing its contents here\n",
    "! cat projects/shape.yml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'train.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# consider this is a simple dataset, train head will be enough.\n",
    "! python train.py -c 0 -p shape --head_only True --lr 1e-3 --batch_size 12 --load_weights weights/efficientdet-d0.pth  --num_epochs 50\n",
    "\n",
    "# the loss will be high at first\n",
    "# don't panic, be patient,\n",
    "# just wait for a little bit longer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'coco_eval.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "% cd ../\n",
    "\n",
    "! python coco_eval.py -c 0 -p shape -w logs/shape/efficientdet-d0_50_3825.pth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Visualize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess\n",
    "\n",
    "compound_coef = 0\n",
    "force_input_size = None  # set None to use default size\n",
    "img_path = 'datasets/shape/val/1000.png'\n",
    "\n",
    "threshold = 0.2\n",
    "iou_threshold = 0.2\n",
    "\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "cudnn.fastest = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "obj_list = ['rectangle', 'circle']\n",
    "\n",
    "# tf bilinear interpolation is different from any other's, just make do\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n",
    "\n",
    "if use_cuda:\n",
    "    x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "else:\n",
    "    x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list))\n",
    "model.load_state_dict(torch.load('logs/shape/efficientdet-d0_50_3825.pth'))\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "if use_float16:\n",
    "    model = model.half()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features, regression, classification, anchors = model(x)\n",
    "\n",
    "    regressBoxes = BBoxTransform()\n",
    "    clipBoxes = ClipBoxes()\n",
    "\n",
    "    out = postprocess(x,\n",
    "                      anchors, regression, classification,\n",
    "                      regressBoxes, clipBoxes,\n",
    "                      threshold, iou_threshold)\n",
    "\n",
    "out = invert_affine(framed_metas, out)\n",
    "\n",
    "for i in range(len(ori_imgs)):\n",
    "    if len(out[i]['rois']) == 0:\n",
    "        continue\n",
    "\n",
    "    for j in range(len(out[i]['rois'])):\n",
    "        (x1, y1, x2, y2) = out[i]['rois'][j].astype(np.int)\n",
    "        cv2.rectangle(ori_imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "        obj = obj_list[out[i]['class_ids'][j]]\n",
    "        score = float(out[i]['scores'][j])\n",
    "\n",
    "        cv2.putText(ori_imgs[i], '{}, {:.3f}'.format(obj, score),\n",
    "                    (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (255, 255, 0), 1)\n",
    "\n",
    "    plt.imshow(ori_imgs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}